# 職務経歴書

## 山下慶倫
<div style="background: url(/Users/rasuharu/Documents/SDIM1593.jpeg) 50% 50% no-repeat; background-size:cover; position: absolute; top: 0; left: 0; width: 100%; height: 100%; display: flex; justify-content: center; align-items: center;">
  <div style="background: rgba(0,0,0,0.5); padding: 30px 40px; font-size: 3.0em; color: #fff;">Yoshinori Yamashita</div>
</div>

<div style="page-break-after: always;"></div>

# 概要

|基本情報||
|---|-----|
|Name|山下慶倫 |
|年齢|32|
|最寄駅|押上|
|GitHub|[yoshinori77](https://github.com/yoshinori77)|

## 業務経験
- データ分析：4年
- データエンジニアリング：3年

<br />

## 言語
- Python：5年
- SQL：4年
- Scala：1年
- JavaScript：1.5年
- Ruby：1.5年

## フレームワーク
- scikit-learn：4年
- Keras：2年
- Pandas：4年
- Flask：2年
- FastAPI：1年
- Ruby on Rails：1年

## データ分析
- 構造化データ：3年
- 時系列データ：2年
- 自然言語データ：0.5年
- 画像データ：0.5年

<div style="page-break-after: always;"></div>

# 概要
## スキル・ツール
- GCP：2.5年
- AWS：3年
- Apache Airflow：1.5年
- Apache Beam: 0.5年
- Apache Spark：1.5年
- BigQuery：1.5年
- Hadoop：1.5年
- Tableau・BI：1.5年
- Docker：2.5年
- Git：5年
- Terraform：0.5年
- PostgreSQL：2年
- MySQL：1.5年
- CI/CD：1年
- 脆弱性診断（DevSecOps）：0.5年
- パフォーマンスチューニング：0.5年

## 強み
- 機械学習・データエンジニアリング両方に対応できます。

## 趣味
- 猫の世話
- 漫画・アニメ
- 美味しいご飯を食べること

<div style="page-break-after: always;"></div>

# 職務経歴

## 2023/8 - 現在：ココナラ

### ポジション

- データエンジニア

### データ整備・ワークフロー構築/移行

- Airflow（Cloud Composer）のワークフローを構築・2系移行
- データリネージ・データ品質チェックの導入
- Looker Studioのメンテナンス
- DevSecOps（CI/CD）で開発効率化
- MLOps（Vertex AI Pipelines）で機械学習の運用強化
- チーム間の連携強化、業務フロー改善

### 開発規模

- R&Dグループ
  - エンジニア 3名
  - PM 1名

### 担当業務

- 要件定義・基本設計・詳細設計
- 実装
- 単体テスト

### スキル

- Python、SQL
- ワークフロー構築（Cloud Composer、Apache Airflow）
- GCP（Cloud Composer、Cloud Storage、BigQuery...）
- Docker
- CI/CD（GitHub Actions）
- Vertex AI Pipelines
- ベクトル近傍探索

### コメント

- これまで経験がある業務が多かったのでリードしつつ、ドメイン知識などわからない部分はフォローしてもらいつつ業務を進めました。
- 開発効率化に向けてチケットのテンプレートを作成したり、CI/CDでテスト・脆弱性チェック・デプロイを自動化したりしました。
- 技術以外の部分も改善しました。インフラチームと私が所属しているグループの間でタスクの連携がスムーズにできていなかったので、業務フローを決めて連携しやすくしました。

<div style="page-break-after: always;"></div>

# 職務経歴
## 2023/1 - 2023/7： アスタミューゼ

### ポジション
- データエンジニア

### 複数プロジェクトのデータ基盤・ワークフロー構築
- Pythonでバッチジョブのロジックを開発（Dataflow）
- AirflowのDAGを構築・整備
- 前処理済みのデータをBigQueryに保存（SQL）
- Cloud Profiler、Logging、Snyk、SAST、データリネージなど便利ツールの導入（モニタリング、脆弱性診断ツールなど）
- IAMの権限管理、Docker・CI/CDなどチームの開発環境の整備

### 開発規模
- エンジニア 3名
- PM 1名

### 担当業務
- 基本設計・詳細設計
- DevOps、実装（ワークフロー構築、ビッグデータ処理）
- 単体テスト・結合テスト

### スキル

- Python、SQL
- ワークフローの構築（Cloud Composer、Apache Airflow）
- スケーラブルなバッチジョブでデータ処理（Dataflow、Apache Beam）
- Sparkのビッグデータ処理
- GCP（Cloud Composer、Cloud Storage、Artifact Registry、Dataproc...）
- Docker、CI/CD（GitLab Runner）

### コメント
- リードエンジニアが抜けるなど不安定な体勢の中、役割を超えて協力し合うチームワークでピンチを乗り切った経験で人間としてもエンジニアとしても成長しました。
- Pythonの型ヒント、GCPの各サービス、CI/CD、Docker、K8sなどモダンな環境で得られた経験値は非常に学びに繋がりました。
- SREのようにシステムパフォーマンスを意識してモニタリングツールを導入したり、脆弱性診断ツールを取り入れるなど積極的に便利なツールを使用して効率化を図りました。
- 人の入れ替わりが激しい職場なので、ドキュメントを書く習慣を心がけ、さらにデータリネージやER図などを取り入れてデータの流れが誰でもわかるようにしました。

<div style="page-break-after: always;"></div>

# 職務経歴
## 2021/2 - 2022/12 : パーソルキャリア株式会社

### ポジション
- データエンジニア
- データサイエンティスト

### 大規模レコメンドシステムの運用保守 & AWS -> GCP移行
- 数億レコードのテーブルを分散処理（Spark）で集計などを行う前処理を実施
- 複数のモデルを用いてレコメンドし、パイプラインをシェルスクリプトで管理（色々ツライのでAWSからGCPに移行）

### 開発規模
- エンジニア 6名
- PM 1名

### 担当業務
- 基本設計・詳細設計
- 実装（パイプライン構築、機械学習モデル構築、ビッグデータ処理）
- 単体テスト・結合テスト

### 使用言語
- Python
- Scala

### スキル
- 学習・推論パイプラインの構築（Cloud Composer、シェルスクリプト）
- Terraformによるシステム構築の自動化
- Scala・Sparkのビッグデータ処理
- GCP（Vertex AI、Cloud Composer、Cloud Storage、Artifact Registry、Dataproc...）
- AWS（EMR、S3...）
- Python（Numpy）で協調フィルタリングのコサイン類似度を算出する際に、行列演算で処理時間を約1/100に高速化

### コメント
- 稼働しているサービスの運用保守は初めてだったので学びが多かったです。
- 深夜対応、開発の切り戻しなどがあり精神的にも鍛えられました。
- AWSとGCP両方の開発を経験し、スキルアップにつながりました。

<div style="page-break-after: always;"></div>

# 職務経歴
## 2020/6 - 2021/1: パーソルキャリア株式会社
### ポジション
- データエンジニア
- サーバーサイドエンジニア

### 適正年収の自動査定サービス（のロジック部分のシステム構築）
- 従来のメンバーシップ型からジョブ型の雇用に切り替わってゆく中で、どの企業も市場の適正年収は大きな関心事です。
- 転職市場の情報とユーザーの職種、業種、年齢などを照らし合わせて、職種におけるグレード・年収を推定することで適正年収の指標を提供しました。

### 開発規模
- エンジニア 3名
- データサイエンティスト 1人
- PM 1名

### 担当業務
- 要件定義・基本設計・詳細設計
- 実装（推論パイプライン構築、API構築、通知・ログ環境の整備）
- 単体テスト・結合テスト

### 使用言語
- Python

### スキル
- 推論システムの構築
- 形態素解析、IF-IDFなど基本的な自然言語処理
- AWS CloudFormationによるシステム構築の自動化
- Amazon API Gateway & AWS Lambdaを利用したAPI構築
- AWS LambdaとAmazon EFSの連携（学習済みの機械学習モデルを配置）
- AWS Cloud Watch Logs を利用したログ環境の整備

### コメント
- これまでアプリケーションを開発経験が少なかったのですが、諸々の事情でほぼ一人で開発を進めることになり、なんとかやり遂げました。
- 最終的には使用しませんでしたが、SQS、SageMaker、Step Functionsなどにも触れました。

<div style="page-break-after: always;"></div>

# 職務経歴
## 2019/9 - 2020/5: フリーランス

### ポジション
- データアナリスト

### ECサービスの顧客分析
- 顧客満足度（NPS）を機械学習で予測して、サービスの改善（特に販売促進）をすることが目的でした。
- これまでのユーザー全てに同じ施策を打つマスマーケティグから、ユーザーごとに施策を分けるターゲットマーケティングへの転換の一歩に微力ながら貢献できたと自負しております。

### 開発規模
- エンジニア 1名
- PM 1名

### 担当業務
- 要件定義
- 実装（可視化、前処理、機械学習モデル構築、評価）
- 単体テスト

### 使用言語
- Python

### スキル
- 構造化データの前処理・特徴量エンジニアリング
- 不均衡データへの対策（Over-sampling、Under-sampling、Calibration）
- BigQuery
- Keras
- scikit-learn

<div style="page-break-after: always;"></div>

# 職務経歴
## 2018/11 - 2019/9: フリーランス

### ポジション
- データアナリスト

### 住宅価格査定ロジック構築
- このプロジェクトでは物件の価格査定を自動化することが目的でした。
- これまでは人が価格査定をするか、機械学習で予測していても精度が低く信頼性が低い問題がありました。
- 以前よりも精度の高い機械学習モデルを構築しました。
- Treasure Data等を利用してETLを行いデータエンジニアリングも行いました。

### 開発規模
- エンジニア 2名
- PM 1名

### 担当業務
- 要件定義
- 実装（可視化、前処理、機械学習モデル構築、評価）
- 単体テスト

### 使用言語
- Python

### スキル
- 構造化データの前処理・特徴量エンジニアリング
- Treasure Data
- Digdagを利用したワークフロー構築
- Embulkを利用したデータ転送
- scikit-learnを用いた機械学習処理
- チーム開発

<div style="page-break-after: always;"></div>

# 職務経歴

## 2017/11 - 2018/10: DATUM STUDIO株式会社

### ポジション
- データアナリスト

### レコメンドシステム構築/WebAPI開発
- アンケートデータからユーザーの求めるアイテムをレコメンドすることで、サービスの価値向上に貢献しました。
- 特に実装フェーズでは前処理、機械学習（分類）、協調フィルタリング、WebAPI構築などを担当しました。
- またダッシュボードの構築も行いました。

### 開発規模
- エンジニア 2名
- PM 1名

### 担当業務
- 要件定義
- 実装（可視化、前処理、機械学習モデル構築、評価）
- 単体テスト

### 使用言語
- Python

### スキル
- Amazon EC2上にレコメンドシステムを構築
- Flaskを利用したWebAPI構築
- Amazon RDS（MariaDB）
- Amazon S3
- scikit-learn
- Tableau
- チーム開発

### コメント
- 業務でAWSを使用したのは初めてだったので勉強になりました。

<div style="page-break-after: always;"></div>

# 職務経歴

## 2017/5 - 2017/10: 株式会社モノゴコロ

### ポジション
- データアナリスト

### 画像認識を用いたサッカー動画解析
- YOLOというディープラーニングアルゴリズムを用いて物体検出を行いサッカー動画から自動で選手のチームを判別するアプリケーションのプロトタイプを作成しました。
- この技術の発展版として、人を検知することで自動で交通量を調査するアプリケーションを構築しました。

### 開発規模
- エンジニア 1名  
- PM 1名

### 担当業務
- 要件定義
- 実装
- 単体テスト

### 使用言語
- Python
- JavaScript

### スキル
- 画像認識
- OpenCV
- D3.js
- Git

### コメント
- 画像認識やデータ分析に触れるきっかけになりました。


<div style="page-break-after: always;"></div>

# 職務経歴

## 2016/11 - 2017/4: 株式会社モノゴコロ

### ポジション
- サーバーサイドエンジニア

### チャットアプリ
- WebSocketを用いてリアルタイムチャットアプリを実装しました。Ruby on Railsを使用しました。

### 開発規模
- エンジニア 1名  
- PM 1名

### 担当業務
- 実装

### 使用言語
- Ruby
- JavaScript

### スキル
- Ruby on Rails
- PostgreSQL
- Git

### コメント
- Railsの使い方やGitなど開発の基本を学びました。


<div style="page-break-after: always;"></div>

# 個人活動

## Kaggle

[プロフィール](https://www.kaggle.com/yamashita)
### 使用言語
- Python

### スキル
- [pandas](https://pandas.pydata.org/)
- [Numpy](https://numpy.org/)
- [SciPy](https://scipy.org/)
- [LightGBM](https://github.com/microsoft/LightGBM)
- [Keras](https://keras.io/)
- [TensorFlow](https://www.tensorflow.org/)
- [scikit-learn](https://scikit-learn.org/stable/)

### 参加コンペ
- JPX Tokyo Stock Exchange Prediction（2022）
  - [GitHub](https://github.com/yoshinori77/jpx_tokyo_market_prediction)
  - 途中で挫折しました...
- Santander Customer Transaction Prediction（2019）
  - 銅メダルでした。 549/8751（7%）
- Data Science Bowl（2019）
  - ダメでした。1215/3493
### コメント
- Kaggleでデータサイエンスのスキルを磨きました。
- まだまだ結果を出せていないので、今後も粘り強くチャレンジします。
